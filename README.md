# 118A-Final-CNM06

Project Title: Evaluating Machine Learning Classifiers: A Simplified Replication of Caruana and Niculescu-Mizil (2006)

Brief Description: This COGS 118A final project replicates key aspects of the seminal work by Caruana and Niculescu-Mizil (2006) on comparing supervised learning algorithms. I implemented and evaluated four popular machine learning algorithms (K-Nearest Neighbors, Random Forests, Gradient Boosting, and Multilayer Perceptron) across four binary classification problems, using three performance metrics: accuracy, F1 score, and area under the ROC curve.
My Role:  

Implementing the machine learning algorithms and evaluation framework
Preprocessing and preparing the datasets
Conducting hyperparameter optimization using GridSearchCV
Analyzing and interpreting the results
Writing a comprehensive report detailing the methodology and findings

Key Skills Demonstrated:
Python (scikit-learn, pandas, numpy)
Machine Learning algorithms: KNN, Random Forests, Gradient Boosting, Multilayer Perceptron
Hyperparameter optimization using GridSearchCV
Cross-validation for model evaluation
Data preprocessing techniques including feature scaling
Performance metrics: Accuracy, F1 score, AUC-ROC

Outcomes and Results: The study found that Gradient Boosting consistently outperformed other algorithms across different metrics and datasets, aligning with findings from the original paper. The project also highlighted the importance of hyperparameter tuning and the impact of feature scaling on model performance. This replication study deepened my understanding of machine learning algorithm behavior across different problem types and the crucial role of proper experimental design in ML research.
